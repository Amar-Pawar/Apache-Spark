{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "*** @Author: Amar Pawar ***\n",
    "\n",
    "*** @Date: 2021-08-23 ***\n",
    "\n",
    "*** @Last Modified by: Amar Pawar ***\n",
    "\n",
    "*** @Last Modified time: 2021-08-23 ***\n",
    "\n",
    "*** @Title : Storing data from spark dataframe to MySQL, and from MySQL to pyspark ***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# *PYSPARK DATAFRAMES FROM CSVs*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql import *\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "appName = \"PySpark to MySQL\"\n",
    "master = \"local\"\n",
    "spark = SparkSession.builder.master(master).appName(appName).getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loadind csv file and creating dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "Df= spark.read.format(\"csv\").option(\"header\",\"true\").load(\"hdfs://localhost:9000/Spark/CpuLogData/*.csv\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## counting total entries in dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "Df.select(\"user_name\",\"DateTime\",\"boot_time\",\"keyboard\",\"mouse\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-------------------+--------------+--------+------+\n",
      "|           user_name|           DateTime|     boot_time|keyboard| mouse|\n",
      "+--------------------+-------------------+--------------+--------+------+\n",
      "|  iamnzm@outlook.com|2019-09-19 08:40:02|0:09:59.262105|     1.0|  32.0|\n",
      "|  iamnzm@outlook.com|2019-09-19 08:45:02|0:14:59.259253|     0.0|   0.0|\n",
      "|  iamnzm@outlook.com|2019-09-19 08:50:01|0:19:58.817858|     0.0|   0.0|\n",
      "|  iamnzm@outlook.com|2019-09-19 08:55:01|0:24:58.366251|    11.0| 900.0|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:00:01|0:29:59.008276|     2.0|  25.0|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:05:01|0:34:58.858791|    37.0| 336.0|\n",
      "|deepshukla292@gma...|2019-09-19 09:05:01|0:05:19.424878|     0.0|  55.0|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:10:01|0:39:58.482956|     0.0| 136.0|\n",
      "|deepshukla292@gma...|2019-09-19 09:10:01|0:10:19.516467|     6.0|1112.0|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:15:02|0:44:59.088574|     0.0|  84.0|\n",
      "|deepshukla292@gma...|2019-09-19 09:15:02|0:15:20.211072|     0.0| 357.0|\n",
      "|markfernandes66@g...|2019-09-19 09:15:01|0:15:37.306887|    20.0| 670.0|\n",
      "|markfernandes66@g...|2019-09-19 09:10:01|0:10:37.345523|    17.0|   0.0|\n",
      "|markfernandes66@g...|2019-09-19 09:20:01|0:20:37.261825|    29.0|1895.0|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:20:02|0:49:59.132879|     0.0|  29.0|\n",
      "|deepshukla292@gma...|2019-09-19 09:20:02|0:20:20.165577|    35.0|3022.0|\n",
      "|markfernandes66@g...|2019-09-19 09:25:01|0:25:37.662065|     2.0| 238.0|\n",
      "|  iamnzm@outlook.com|2019-09-19 09:25:01|0:54:58.698849|     0.0|   0.0|\n",
      "|deepshukla292@gma...|2019-09-19 09:25:01|0:25:19.999863|    20.0|1330.0|\n",
      "|markfernandes66@g...|2019-09-19 09:30:01|0:30:37.246632|     5.0|  39.0|\n",
      "+--------------------+-------------------+--------------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating temp view to perform sql quieries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "Df.createOrReplaceTempView(\"log_data\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-23 20:13:06,829 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Query to fetch total entries for each person"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "avg_hr_df= spark.sql(\"select user_name ,count('') as total from log_data where keyboard !=0.0 or mouse!=0.0 group by user_name\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "avg_hr_df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+\n",
      "|           user_name|total|\n",
      "+--------------------+-----+\n",
      "|salinabodale73@gm...|  440|\n",
      "|sharlawar77@gmail...|  457|\n",
      "|rahilstar11@gmail...|  399|\n",
      "|deepshukla292@gma...|  475|\n",
      "|  iamnzm@outlook.com|  459|\n",
      "|markfernandes66@g...|  389|\n",
      "|damodharn21@gmail...|  191|\n",
      "|bhagyashrichalke2...|  361|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Storing data from spark dataframe to MySQL table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import os\n",
    "from logging_handler import logger\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "database=os.getenv(\"DB_NAME\")\n",
    "user=os.getenv(\"DB_USER\")\n",
    "password=os.getenv(\"DB_PASSWORD\")\n",
    "host=os.getenv(\"DB_HOST\")\n",
    "auth_plugin=os.getenv('AUTH_PLUGIN')\n",
    "logger.info(database)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "try:\n",
    "    jdb_curl = f\"jdbc:mysql://localhost:3306/\" + database\n",
    "    #write the dataframe into a sql table\n",
    "    avg_hr_df.write.format(\"jdbc\").option(\"url\",jdb_curl) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\",\"record_count\") \\\n",
    "        .option(\"user\",user).option(\"password\",password).save()\n",
    "    logger.info(\"written to mysql db successfully\")\n",
    "except Exception as e:\n",
    "    logger.info(e)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fetching data from MySQL table to pyspark dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "try:\n",
    "    conn = mysql.connector.connect(user=user, database=database,\n",
    "                                password=password,\n",
    "                                host=host,\n",
    "                                port=3306,\n",
    "                                    auth_plugin=auth_plugin)\n",
    "    cursor = conn.cursor()\n",
    "    query = \"SELECT * FROM record_count\"\n",
    "    #Create a pandas dataframe\n",
    "    pdf = pd.read_sql(query, con=conn)\n",
    "    conn.close()\n",
    "\n",
    "    # Convert Pandas dataframe to spark DataFrame\n",
    "    df = spark.createDataFrame(pdf)\n",
    "except Exception as e:\n",
    "    logger.info(e)\n",
    "\n",
    "df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-----+\n",
      "|           user_name|total|\n",
      "+--------------------+-----+\n",
      "|salinabodale73@gm...|  440|\n",
      "|sharlawar77@gmail...|  457|\n",
      "|rahilstar11@gmail...|  399|\n",
      "|deepshukla292@gma...|  475|\n",
      "|  iamnzm@outlook.com|  459|\n",
      "|markfernandes66@g...|  389|\n",
      "|damodharn21@gmail...|  191|\n",
      "|bhagyashrichalke2...|  361|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}